## **flutter-ai-chat-backend**
This is the backend source code for a **multifunctional AI chat application** built with **Flask + PostgreSQL** and integrated with several AI services including **OpenAI API, Replicate API (Google/Imagen-4), and Runway API (gen4_turbo)**.

The server handles a variety of AI-driven tasks for the frontend (Flutter, iOS, Android), including:

- Text-based chat responses using GPT models
- Image generation and analysis
- Audio transcription and understanding
- Video generation from images and text prompts
- Multimodal processing that combines text, images, and audio

It provides a unified API to send user inputs and receive AI-generated content, making it a versatile backend for chatbots, virtual avatars, and multimedia AI applications.


## **Setup**
### Step 1. Create Database Table
```sql
CREATE TABLE chat_messages (
    id SERIAL PRIMARY KEY,
    user_id VARCHAR(50) NOT NULL,
    message TEXT NOT NULL,
    reply TEXT,
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
);
```

### Step 2. Configure Environment Variables
Create a .env file in the project root:
.env
```
DB_NAME=postgres
DB_USER=postgres
DB_PASSWORD=hoge
DB_HOST=localhost
DB_PORT=5432
OPENAI_API_KEY=
REPLICATE_API_TOKEN=
RUNWAY_API_KEY=
```

### Step 3. Install Dependencies
```bash
python3 -m venv venv
source venv/bin/activate
pip install flask psycopg2-binary openai python-dotenv gTTS pydub numpy replicate runwayml
```

### Step 4. Run Server
```
python3 app.py
```

The Flask server will start at:<br>
ğŸ‘‰ http://127.0.0.1:5000/chat<br>
ğŸ‘‰ http://127.0.0.1:5000/avatar<br>
ğŸ‘‰ http://127.0.0.1:5000/understand_audio<br>
ğŸ‘‰ http://127.0.0.1:5000/generate_image<br>
ğŸ‘‰ http://127.0.0.1:5000/understand_image<br>
ğŸ‘‰ http://127.0.0.1:5000/generate_video<br>
ğŸ‘‰ http://127.0.0.1:5000/multimodal<br>


### Step 5. Test API
#### 1. Chat API
```
curl -X POST http://127.0.0.1:5000/chat \
  -H "Content-Type: application/json" \
  -d '{"user_id": "12345", "message": "ãŠã¯ã‚ˆã†ï¼"}'
```
Response:

```
{
  "reply": "ãŠã¯ã‚ˆã†ã”ã–ã„ã¾ã™ï¼å…ƒæ°—ã§ã™ã‹ï¼Ÿä½•ã‹ãŠæ‰‹ä¼ã„ã§ãã‚‹ã“ã¨ãŒã‚ã‚Šã¾ã™ã‹ï¼Ÿ"
}
```

#### 2. Avatar API (Chat + TTS + Lip-Sync Data)
```
curl -X POST http://127.0.0.1:5000/avatar \
  -H "Content-Type: application/json" \
  -d '{"text": "ãŠã¯ã‚ˆã†ï¼"}'
```
Response (example):

```
{
  "text": "ãŠã¯ã‚ˆã†ã”ã–ã„ã¾ã™ï¼ä»Šæ—¥ã‚‚é ‘å¼µã‚Šã¾ã—ã‚‡ã†ï¼",
  "audio_url": "http://127.0.0.1:5000/audio/output_12345abcd.wav",
  "lip_sync": [0.0, 0.12, 0.24, ...]
}
```
You can fetch the generated audio file directly:
```
curl -O http://127.0.0.1:5000/audio/output_12345abcd.wav
```

#### 3. Avatar API (Chat + TTS + Lip-Sync Data)
Understand the content of an audio file and return a text answer using **OpenAI GPT-4o-mini**.
```
curl -X POST http://127.0.0.1:5000/understand_audio \
  -H "Content-Type: application/json" \
  -d '{
        "audio_url": "https://hpscript.s3.ap-northeast-1.amazonaws.com/voice.wav"
      }'
```
Response (example):
```
{"transcript": "AIãƒãƒ£ãƒƒãƒˆã§ã‚ˆãèã‹ã‚Œã‚‹ã“ã¨ã¯ä½•ã§ã™ã‹?", "answer": "AIãƒãƒ£ãƒƒãƒˆã§ã‚ˆãèã‹ã‚Œã‚‹ã“ã¨ã¯ã„ãã¤ã‹ã‚ã‚Šã¾ã™ãŒã€ä»¥ä¸‹ã®ã‚ˆã†ãªè³ªå•ãŒä¸€èˆ¬çš„ã§ã™ã€‚\n\n1. **æƒ…å ±æä¾›**:\n   - æ­´å²çš„ãªå‡ºæ¥äº‹ã‚„æ–‡åŒ–ã«é–¢ã™ã‚‹è³ªå•\n   - ç§‘å­¦ã‚„æŠ€è¡“ã«é–¢ã™ã‚‹åŸºæœ¬çš„ãªæƒ…å ±\n\n2. **åŠ©è¨€ã‚„ææ¡ˆ**:\n   - å¥åº·ã‚„ç”Ÿæ´»ã«é–¢ã™ã‚‹ã‚¢ãƒ‰ãƒã‚¤ã‚¹\n   - å­¦ç¿’æ–¹æ³•ã‚„ã‚­ãƒ£ãƒªã‚¢ã«é–¢ã™ã‚‹ç›¸è«‡\n\n3. **ãƒˆãƒ©ãƒ–ãƒ«ã‚·ãƒ¥ãƒ¼ãƒ†ã‚£ãƒ³ã‚°**:\n   - ã‚³ãƒ³ãƒ”ãƒ¥ãƒ¼ã‚¿ã‚„ã‚½ãƒ•ãƒˆã‚¦ã‚§ã‚¢ã®å•é¡Œè§£æ±º\n   - æ—¥å¸¸ç”Ÿæ´»ã§ã®ãƒˆãƒ©ãƒ–ãƒ«ã¸ã®å¯¾å‡¦æ³•\n\n4. **ã‚¨ãƒ³ã‚¿ãƒ¼ãƒ†ã‚¤ãƒ¡ãƒ³ãƒˆ**:\n   - æ˜ ç”»ã‚„æœ¬ã®ãŠã™ã™ã‚\n   - ã‚¸ãƒ§ãƒ¼ã‚¯ã‚„ã‚¯ã‚¤ã‚º\n\n5. **é›‘å­¦ã‚„è±†çŸ¥è­˜**:\n   - é¢ç™½ã„äº‹å®Ÿã‚„ trivia\n   - ç‰¹å®šã®ãƒ†ãƒ¼ãƒã«é–¢ã™ã‚‹è©³ç´°\n\n6. **æŠ€è¡“çš„ãªè³ªå•**:\n   - ãƒ—ãƒ­ã‚°ãƒ©ãƒŸãƒ³ã‚°ã‚„ã‚³ãƒ³ãƒ”ãƒ¥ãƒ¼ã‚¿ã‚µã‚¤ã‚¨ãƒ³ã‚¹ã«é–¢ã™ã‚‹è³ªå•\n   - AIã‚„æ©Ÿæ¢°å­¦ç¿’ã®ä»•çµ„ã¿\n\nã“ã‚Œã‚‰ã®è³ªå•ã«å¯¾ã—ã¦ã€ã§ãã‚‹é™ã‚Šæ­£ç¢ºã‹ã¤è©³ç´°ãªæƒ…å ±ã‚’æä¾›ã™ã‚‹ã“ã¨ãŒæ±‚ã‚ã‚‰ã‚Œã¾ã™ã€‚", "metadata": {"model": "gpt-4o-mini", "created": "20250824032605"}}
```
- The `transcript` field contains the text of the spoken content in the audio file.  
- The `answer` field contains the model's response to the content of the audio.  
- Metadata includes the model name and timestamp of generation.  

#### 4. Generate Image API
Generate images from text prompts using **Replicate API (Google/Imagen-4)**.
```
curl -X POST http://127.0.0.1:5000/generate_image \
  -H "Content-Type: application/json" \
  -d '{"prompt": "an astronaut riding a horse on mars, hd, dramatic lighting"}'
```
Response (example):
```
{
  "prompt": "an astronaut riding a horse on mars, hd, dramatic lighting",
  "image_url": "http://127.0.0.1:5000/image/image_20250823034938.png",
  "metadata": {
    "model": "google/imagen-4",
    "created": "20250823034938"
  }
}
```
<img src="https://hpscript.s3.ap-northeast-1.amazonaws.com/astronaut_house.png" alt="sample image" width="50%">
- The generated images are saved in the images/ directory on the server.
- You can access them directly via the returned image_url.

#### 5. Understand Image API
Analyze an image and ask questions about it using OpenAI GPT-4o-mini.
```
curl -X POST http://127.0.0.1:5000/understand_image \
  -H "Content-Type: application/json" \
  -d '{
        "question": "ã“ã®çŠ¬ã®çŠ¬ç¨®ã¯ä½•ã§ã™ã‹ï¼Ÿ",
        "image_url": "https://hpscript.s3.ap-northeast-1.amazonaws.com/dog.jpg"
      }'
```
Response (example):
```
{
  "question": "ã“ã®çŠ¬ã®çŠ¬ç¨®ã¯ä½•ã§ã™ã‹ï¼Ÿ",
  "answer": "ã“ã®çŠ¬ã¯ã‚¢ãƒ©ã‚¹ã‚«ãƒ³ãƒ»ãƒãƒ©ãƒŸãƒ¥ãƒ¼ãƒˆã¨ã„ã†çŠ¬ç¨®ã®ã‚ˆã†ã§ã™ã€‚å¤§å‹ã§åŠ›å¼·ã„çŠ¬ã§ã€ä¸»ã«ãã‚Šå¼•ãã¨ã—ã¦é£¼è‚²ã•ã‚Œã¦ã„ã¾ã™ã€‚ç‰¹å¾´çš„ãªæ¯›è‰²ã¨ç©ã‚„ã‹ãªæ€§æ ¼ãŒé­…åŠ›ã§ã™ã€‚",
  "metadata": {
    "model": "gpt-4o-mini",
    "created": "20250824022922"
  }
}
```
<img src="https://hpscript.s3.ap-northeast-1.amazonaws.com/dog.jpg" width="50%">
- The API accepts any image URL and a question in Japanese or English.<br>
- The response contains the AIâ€™s answer and metadata including the model used and timestamp.

#### 6. Generate Video API
Generate short videos from an image + text prompt using Runway API (gen4_turbo).
```
curl -X POST http://127.0.0.1:5000/generate_video \
  -H "Content-Type: application/json" \
  -d '{
        "prompt": "å®‡å®™èˆ¹ãŒé£›ã¶è¡—",
        "prompt_image": "https://hpscript.s3.ap-northeast-1.amazonaws.com/space.jpg",
        "duration": 5
      }'
```
Response (example):
```
{
  "prompt": "å®‡å®™èˆ¹ãŒé£›ã¶è¡—",
  "video_url": "http://127.0.0.1:5000/video/video_20250823181941.mp4",
  "metadata": {
    "model": "gen4_turbo",
    "duration": 5,
    "created": "20250823181941"
  }
}
```
<a href="https://hpscript.s3.ap-northeast-1.amazonaws.com/spacecraft.mp4">
  <img src="https://hpscript.s3.ap-northeast-1.amazonaws.com/space_craft.png" width="50%">
</a><br>

- The generated video is saved in the videos/ directory on the server.
- You can access it directly via the returned video_url.
- Note: duration must be 5 or 10 seconds due to API constraints.

#### 7. Multimodal API (Text + Image + Audio)
Analyze text, images, and audio simultaneously using OpenAI GPT-4o-mini.
- text â†’ question or instruction
- image_url â†’ image URL
- audio_url â†’ audio URL (mp3/wav)
```
curl -X POST http://127.0.0.1:5000/multimodal \
  -H "Content-Type: application/json" \
  -d '{
        "text": "çŒ«ã«ã¤ã„ã¦èª¬æ˜ã—ã¦ãã ã•ã„",
        "image_url": "https://hpscript.s3.ap-northeast-1.amazonaws.com/cat.jpg",
        "audio_url": "https://hpscript.s3.ap-northeast-1.amazonaws.com/cat_voice.mp3"
      }'
```
Response (example):
```
{
  "image_analysis": {
    "question": "çŒ«ã«ã¤ã„ã¦èª¬æ˜ã—ã¦ãã ã•ã„",
    "answer": "çŒ«ã¯ã€å°å‹ã®éœŠé•·é¡ã«å±ã™ã‚‹å“ºä¹³å‹•ç‰©ã§ã€ç‰¹ã«å®¶ç•œã¨ã—ã¦äººæ°—ãŒã‚ã‚Šã¾ã™ã€‚å½¼ã‚‰ã¯ç‹¬ç«‹å¿ƒãŒå¼·ãã€è­¦æˆ’å¿ƒã‚‚ã‚ã‚‹ä¸€æ–¹ã§ã€æ„›æƒ…æ·±ãé£¼ã„ä¸»ã¨å¼·ã„çµ†ã‚’çµã¶ã“ã¨ãŒã§ãã¾ã™ã€‚\n\n### åŸºæœ¬çš„ãªç‰¹å¾´\n\n- **ä½“å‹**: ã‚¹ãƒªãƒ ã§æŸ”è»Ÿã€ä¿Šæ•ãªä½“ã‚’æŒã¡ã€ç´„3ã€œ5ã‚­ãƒ­ã‚°ãƒ©ãƒ ã‹ã‚‰10ã‚­ãƒ­ã‚°ãƒ©ãƒ ç¨‹åº¦ã®å¤§ãã•ã€‚\n- **è¢«æ¯›**: çŸ­æ¯›ç¨®ã¨é•·æ¯›ç¨®ãŒã‚ã‚Šã€ã•ã¾ã–ã¾ãªè‰²ã¨æ¨¡æ§˜ãŒã‚ã‚Šã¾ã™ã€‚\n- **æ„Ÿè¦š**: å„ªã‚ŒãŸè¦–è¦šã¨è´è¦šã‚’æŒã¡ã€å¤œè¡Œæ€§ã§ã€å¤œé–“ã§ã‚‚ã‚ˆãè¦‹ãˆã¾ã™ã€‚\n\n### è¡Œå‹•\n\n- **ç¤¾ä¼šæ€§**: ä¸€èˆ¬çš„ã«ã€çŒ«ã¯ç‹¬ã‚Šã§éã”ã™ã®ãŒå¥½ãã§ã™ãŒã€ä¸€ç·’ã«éŠã‚“ã ã‚Šéã”ã—ãŸã‚Šã™ã‚‹ã“ã¨ã§ã€ç¤¾ä¼šçš„ãªçµã³ã¤ãã‚’å½¢æˆã—ã¾ã™ã€‚\n- **ç¿’æ€§**: ç²ç‰©ã‚’æ•ã¾ãˆã‚‹ãŸã‚ã®ç‹©çŒŸæœ¬èƒ½ã‚’æŒã¡ã€éŠã¶ã“ã¨ã‚’é€šã˜ã¦ã“ã®æœ¬èƒ½ã‚’ç™ºæ®ã—ã¾ã™ã€‚\n\n### é£¼ã„æ–¹\n\n- **é£¼è‚²ç’°å¢ƒ**: å®¶åº­å†…ã§ã®é£¼è‚²ã«é©ã—ã€å¿…è¦ãªç’°å¢ƒã‚’æ•´ãˆã‚‹ã“ã¨ã§å¥åº·çš„ã«è‚²ã¦ã‚‹ã“ã¨ãŒã§ãã¾ã™ã€‚\n- **é£Ÿäº‹**: è‚‰é£Ÿæ€§ã§ã€çŒ«ç”¨ã®é¤Œã‚’æä¾›ã™ã‚‹ã“ã¨ãŒé‡è¦ã§ã™ã€‚\n\nå¤šãã®å®¶åº­ã§æ„›ã•ã‚Œã‚‹å­˜åœ¨ã§ã‚ã‚Šã€ã‚¹ãƒˆãƒ¬ã‚¹ã‚’è»½æ¸›ã™ã‚‹åŠ¹æœã‚‚ã‚ã‚‹ã¨ã•ã‚Œã¦ã„ã¾ã™ã€‚",
    "metadata": {
      "model": "gpt-4o-mini",
      "created": "20250824202654"
    }
  },
  "audio_analysis": {
    "transcript": "Meow!",
    "answer": "ã€Œãƒ‹ãƒ£ãƒ¼ï¼ã€ã¨ã„ã†çŒ«ã®é³´ãå£°ã‚’è¡¨ç¾ã—ã¦ã„ã¾ã™ã€‚ã“ã‚Œã¯çŒ«ãŒè‡ªåˆ†ã®å­˜åœ¨ã‚’çŸ¥ã‚‰ã›ãŸã‚Šã€ä½•ã‹ã‚’è¦æ±‚ã—ãŸã‚Šã™ã‚‹éš›ã«ã‚ˆãä½¿ã†éŸ³ã§ã™ã€‚",
    "metadata": {
      "model": "gpt-4o-mini",
      "created": "20250824202658"
    }
  },
  "text_response": {
    "answer": "ãƒ†ã‚­ã‚¹ãƒˆå…¥åŠ›ã€çŒ«ã«ã¤ã„ã¦èª¬æ˜ã—ã¦ãã ã•ã„ã€ã«åŸºã¥ãå¿œç­”ã—ã¾ã™ã€‚"
  }
}
```
- image_analysis.answer â†’ AI explanation based on the image
- audio_analysis.transcript â†’ Transcribed text from the audio
- audio_analysis.answer â†’ Japanese/English explanation of the audio content
- text_response.answer â†’ Response based on the input text
- Supports Japanese and English, with proper UTF-8 encoding

## Note 
- The frontend (Flutter app) will call this backend API.
- Make sure PostgreSQL is running before starting the server.
- Audio files are generated under the audio/ directory and served via /audio/<filename>.
- Image files are generated under the images/ directory and served via /images/<filename>.
- Video files are generated under the videos/ directory and served via /video/<filename>.

## Frontend Demo
<p align="left">
<img src="https://hpscript.s3.ap-northeast-1.amazonaws.com/ios_demo.png" alt="sample image" width="30%">
</p>